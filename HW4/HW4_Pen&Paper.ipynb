{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\IST\\LEIC 2022-\\2 Ano\\2 Ano 1 Trimestre\\Aprendizagem\\Homeworks\\Homework 1\\Codigo\\HW4_Pen&Paper.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/IST/LEIC%202022-/2%20Ano/2%20Ano%201%20Trimestre/Aprendizagem/Homeworks/Homework%201/Codigo/HW4_Pen%26Paper.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# E-step: Compute posterior probabilities\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/IST/LEIC%202022-/2%20Ano/2%20Ano%201%20Trimestre/Aprendizagem/Homeworks/Homework%201/Codigo/HW4_Pen%26Paper.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_clusters):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/IST/LEIC%202022-/2%20Ano/2%20Ano%201%20Trimestre/Aprendizagem/Homeworks/Homework%201/Codigo/HW4_Pen%26Paper.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     posterior[:, k] \u001b[39m=\u001b[39m pi[k] \u001b[39m*\u001b[39m multivariate_normal\u001b[39m.\u001b[39;49mpdf(data, mean\u001b[39m=\u001b[39;49mmeans[k], cov\u001b[39m=\u001b[39;49mcovs[k])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/IST/LEIC%202022-/2%20Ano/2%20Ano%201%20Trimestre/Aprendizagem/Homeworks/Homework%201/Codigo/HW4_Pen%26Paper.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m posterior \u001b[39m=\u001b[39m posterior \u001b[39m/\u001b[39m posterior\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/IST/LEIC%202022-/2%20Ano/2%20Ano%201%20Trimestre/Aprendizagem/Homeworks/Homework%201/Codigo/HW4_Pen%26Paper.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# M-step: Update parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_multivariate.py:588\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[1;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[0;32m    586\u001b[0m dim, mean, cov_object \u001b[39m=\u001b[39m params\n\u001b[0;32m    587\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_quantiles(x, dim)\n\u001b[1;32m--> 588\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logpdf(x, mean, cov_object))\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(cov_object\u001b[39m.\u001b[39mrank \u001b[39m<\u001b[39m dim):\n\u001b[0;32m    590\u001b[0m     out_of_bounds \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mcov_object\u001b[39m.\u001b[39m_support_mask(x\u001b[39m-\u001b[39mmean)\n",
      "File \u001b[1;32mc:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_multivariate.py:531\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._logpdf\u001b[1;34m(self, x, mean, cov_object)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Log of the multivariate normal probability density function.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \n\u001b[0;32m    514\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \n\u001b[0;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    530\u001b[0m log_det_cov, rank \u001b[39m=\u001b[39m cov_object\u001b[39m.\u001b[39mlog_pdet, cov_object\u001b[39m.\u001b[39mrank\n\u001b[1;32m--> 531\u001b[0m dev \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39;49m mean\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m dev\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    533\u001b[0m     log_det_cov \u001b[39m=\u001b[39m log_det_cov[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3) (2,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Given data points\n",
    "data = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "\n",
    "# Given parameters\n",
    "pi = np.array([0.5, 0.5])\n",
    "p = np.array([0.3, 0.7])\n",
    "N1 = {\n",
    "    'mean': np.array([1, 1]),\n",
    "    'cov': np.array([[2, 0.5], [0.5, 2]])\n",
    "}\n",
    "N2 = {\n",
    "    'mean': np.array([0, 0]),\n",
    "    'cov': np.array([[1.5, 1], [1, 1.5]])\n",
    "}\n",
    "\n",
    "# Initialize arrays to store posterior probabilities\n",
    "posterior_probs = np.zeros((len(data), 2))\n",
    "\n",
    "# E-step: Compute posterior probabilities\n",
    "for i in range(len(data)):\n",
    "    likelihood1 = p[0]**data[i, 0] * (1-p[0])**(1-data[i, 0])\n",
    "    likelihood2 = p[1]**data[i, 0] * (1-p[1])**(1-data[i, 0])\n",
    "    \n",
    "    likelihood_gaussian1 = multivariate_normal.pdf(data[i, 1:], mean=N1['mean'], cov=N1['cov'])\n",
    "    likelihood_gaussian2 = multivariate_normal.pdf(data[i, 1:], mean=N2['mean'], cov=N2['cov'])\n",
    "\n",
    "    denominator = pi[0] * likelihood1 * likelihood_gaussian1 + pi[1] * likelihood2 * likelihood_gaussian2\n",
    "    \n",
    "    posterior_probs[i, 0] = (pi[0] * likelihood1 * likelihood_gaussian1) / denominator\n",
    "    posterior_probs[i, 1] = (pi[1] * likelihood2 * likelihood_gaussian2) / denominator\n",
    "\n",
    "print(\"Posterior Probabilities:\\n\", posterior_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior for Cluster 1: 0.20697271237150058\n",
      "Posterior for Cluster 2: 0.7930272876284995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, bernoulli\n",
    "\n",
    "# Given model parameters\n",
    "π1 = 0.5\n",
    "π2 = 0.5\n",
    "p1 = 0.3\n",
    "p2 = 0.7\n",
    "μ1 = np.array([1, 1])\n",
    "Σ1 = np.array([[2, 0.5], [0.5, 2]])\n",
    "μ2 = np.array([0, 0])\n",
    "Σ2 = np.array([[1.5, 1], [1, 1.5]])\n",
    "\n",
    "# New observation\n",
    "x_new = np.array([1, 0.3, 0.7])\n",
    "\n",
    "# Calculate the likelihoods for the new observation\n",
    "likelihood_cluster1 = (\n",
    "    bernoulli.pmf(1, p1) *\n",
    "    multivariate_normal.pdf(x_new[1:], mean=μ1, cov=Σ1)\n",
    ")\n",
    "\n",
    "likelihood_cluster2 = (\n",
    "    bernoulli.pmf(1, p2) *\n",
    "    multivariate_normal.pdf(x_new[1:], mean=μ2, cov=Σ2)\n",
    ")\n",
    "\n",
    "# Calculate unnormalized posteriors\n",
    "unnormalized_posterior1 = π1 * likelihood_cluster1\n",
    "unnormalized_posterior2 = π2 * likelihood_cluster2\n",
    "\n",
    "# Normalize the posteriors\n",
    "posterior1 = unnormalized_posterior1 / (unnormalized_posterior1 + unnormalized_posterior2)\n",
    "posterior2 = unnormalized_posterior2 / (unnormalized_posterior1 + unnormalized_posterior2)\n",
    "\n",
    "print(\"Posterior for Cluster 1:\", posterior1)\n",
    "print(\"Posterior for Cluster 2:\", posterior2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score of the larger cluster under Manhattan distance: 0.7916666666666665\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Given observations\n",
    "observations = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "\n",
    "# Given parameters\n",
    "pi = np.array([0.5, 0.5])\n",
    "p = np.array([0.3, 0.7])\n",
    "mean = np.array([[1, 1], [0, 0]])\n",
    "cov = np.array([[[2, 0.5], [0.5, 2]], [[1.5, 1], [1, 1.5]]])\n",
    "\n",
    "# Step 1: Calculate the likelihood of each observation belonging to each cluster\n",
    "likelihoods = np.zeros((len(observations), len(pi)))\n",
    "for i in range(len(observations)):\n",
    "    for j in range(len(pi)):\n",
    "        likelihoods[i, j] = pi[j] * p[j] ** observations[i, 0] * (1 - p[j]) ** (1 - observations[i, 0]) * \\\n",
    "                            multivariate_normal.pdf(observations[i, 1:], mean[j], cov[j])\n",
    "\n",
    "# Step 2: Assign each observation to the cluster with the highest likelihood\n",
    "assignments = np.argmax(likelihoods, axis=1)\n",
    "\n",
    "# Now, we have assigned each observation to a cluster (0 or 1).\n",
    "\n",
    "# Step 3: Calculate the Manhattan distance between each observation and all other observations in the same cluster\n",
    "manhattan_distances = np.zeros((len(observations), len(observations)))\n",
    "for i in range(len(observations)):\n",
    "    for j in range(len(observations)):\n",
    "        manhattan_distances[i, j] = np.abs(observations[i] - observations[j]).sum()\n",
    "\n",
    "# Step 4: Calculate the average distance of each observation to all other observations in the same cluster\n",
    "average_distances_same_cluster = np.zeros(len(observations))\n",
    "for i in range(len(observations)):\n",
    "    same_cluster_indices = np.where(assignments == assignments[i])[0]\n",
    "    same_cluster_distances = manhattan_distances[i, same_cluster_indices]\n",
    "    average_distances_same_cluster[i] = np.mean(same_cluster_distances)\n",
    "\n",
    "# Step 5: Calculate the average distance of each observation to all observations in the other cluster\n",
    "average_distances_other_cluster = np.zeros(len(observations))\n",
    "for i in range(len(observations)):\n",
    "    other_cluster_indices = np.where(assignments != assignments[i])[0]\n",
    "    other_cluster_distances = manhattan_distances[i, other_cluster_indices]\n",
    "    average_distances_other_cluster[i] = np.mean(other_cluster_distances)\n",
    "\n",
    "# Step 6: Calculate the silhouette score for each observation\n",
    "silhouette_scores = (average_distances_other_cluster - average_distances_same_cluster) / \\\n",
    "                    np.maximum(average_distances_other_cluster, average_distances_same_cluster)\n",
    "\n",
    "# Step 7: Calculate the average silhouette score for the larger cluster\n",
    "larger_cluster = np.argmax([np.sum(assignments == 0), np.sum(assignments == 1)])\n",
    "average_silhouette_larger_cluster = np.mean(silhouette_scores[assignments == larger_cluster])\n",
    "\n",
    "print(\"Silhouette score of the larger cluster under Manhattan distance:\", average_silhouette_larger_cluster)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
